{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-29 21:43:27.717233: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-29 21:43:28.283044: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Import\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from joblib import load\n",
    "from src.general.functions_time import tic, toc, get_timestamp\n",
    "from src.data.functions_training_data import load_processed_data\n",
    "from src.evaluation.functions_prediction import get_multi_dim_prediction\n",
    "from src.general.functions_report import report_prediction_random_forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set tensorflow to GPU-only (data is stored as tensors even when tf is not used)\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory:  /home/jupyter-sfaatz\n"
     ]
    }
   ],
   "source": [
    "# Change working directory\n",
    "# working_directory = 'c:/Users/Steph/OneDrive - Universität Bayreuth/Masterarbeit/03_Programmierung/remaining_trace_prediction_master_thesis_stephan_faatz'\n",
    "working_directory = '/home/jupyter-sfaatz/'\n",
    "os.chdir(working_directory)\n",
    "print(\"Working directory: \", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-29 21:44:26.976706: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2023-12-29 21:44:26.976751: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:168] retrieving CUDA diagnostic information for host: jupyter-ext-wi\n",
      "2023-12-29 21:44:26.976756: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:175] hostname: jupyter-ext-wi\n",
      "2023-12-29 21:44:26.977001: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:199] libcuda reported version is: 535.54.3\n",
      "2023-12-29 21:44:26.977019: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:203] kernel reported version is: 535.54.3\n",
      "2023-12-29 21:44:26.977022: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:309] kernel version seems to match DSO: 535.54.3\n"
     ]
    }
   ],
   "source": [
    "# Set path variables\n",
    "path_raw = 'data/raw/'\n",
    "path_interim = 'data/interim/'\n",
    "path_benchmark = 'data/benchmark/'\n",
    "path_data = 'data/processed/'\n",
    "path_control = 'data/control/'\n",
    "path_predictions = 'data/predictions/'\n",
    "path_models = 'models/'\n",
    "path_reports = 'reports/'\n",
    "\n",
    "# Initalize variables\n",
    "filename_variables = 'variables_helpdesk_true.pkl'\n",
    "\n",
    "with open(path_control + filename_variables, 'rb') as file:\n",
    "    variables = pickle.load(file)\n",
    "\n",
    "# timestamp\n",
    "variables['rf_timestamp_prediction_start'] = get_timestamp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "x_train_tensor shape:  (26332, 12)\n",
      "y_train_tensor shape:  (26332, 1)\n",
      "x_val_tensor shape:    (5643, 12)\n",
      "y_val_tensor shape:    (5643, 1)\n",
      "x_test_tensor shape:   (5643, 12)\n",
      "y_test_tensor shape:  (5643, 12)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load benchmark data\n",
    "x_train, y_train, x_val, y_val, x_test, y_test = load_processed_data(path_benchmark + variables['filename_benchmark_dataset'], tensor = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "rf = load(path_models +'random_forest/'+ variables['random_forest_model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:   0%|          | 0/12 [00:00<?, ?it/s][Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "Predicting:  17%|█▋        | 2/12 [00:00<00:00, 13.20it/s][Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "Predicting:  33%|███▎      | 4/12 [00:00<00:00, 14.02it/s][Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "Predicting:  50%|█████     | 6/12 [00:00<00:00, 14.45it/s][Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "Predicting:  67%|██████▋   | 8/12 [00:00<00:00, 14.42it/s][Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "Predicting:  83%|████████▎ | 10/12 [00:00<00:00, 14.59it/s][Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "Predicting: 100%|██████████| 12/12 [00:00<00:00, 14.34it/s]\n",
      "Cleaning predictions: 100%|██████████| 5643/5643 [00:00<00:00, 1400003.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Elapsed time: 0.849087 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Get multidimensional predictions\n",
    "tic()\n",
    "y_pred = get_multi_dim_prediction(rf, x_test, variables['mapping'])\n",
    "variables['elapsed_time_predictions_rf'] = toc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to  data/predictions/predictions_helpdesk_true.npz\n"
     ]
    }
   ],
   "source": [
    "# Save predictions\n",
    "data_predictions = np.load(path_predictions + variables['filename_predictions'])\n",
    "data_predictions_copy = dict(data_predictions)\n",
    "data_predictions_copy['y_pred_rf'] = y_pred\n",
    "data_predictions_copy['y_test_benchmark'] = y_test\n",
    "np.savez(path_predictions + variables['filename_predictions'], **data_predictions_copy)\n",
    "print(\"Predictions saved to \", path_predictions + variables['filename_predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary:\n",
      "\n",
      "\n",
      "Dataset:                 helpdesk.csv\n",
      "Filename interim data:   interim_data_helpdesk_true.npz\n",
      "Filename processed data: preprocessed_data_helpdesk_true.npz\n",
      "Filename variables:      variables_helpdesk_true.pkl\n",
      "\n",
      "\n",
      "vocab (first 6):         ['<pad>' '<unk>' '<start>' '<end>' 'Assign-seriousness'\n",
      " 'Take-in-charge-ticket']\n",
      "vocab_size:              36\n",
      "max_length_trace:        6\n",
      "num_traces:              4255\n",
      "num_ex_activities:       18809\n",
      "num_features:            2\n",
      "features:                ['concept:name', 'org:resource']\n",
      "interleave:              True\n",
      "\n",
      "\n",
      "Samples in training:     (26332, 14)\n",
      "Samples in validation:   (5643, 14)\n",
      "Samples in test:         (5643, 14)\n",
      "\n",
      "\n",
      "Prediction Random Forest:\n",
      "Elapsed time:            0.8102619647979736\n",
      "Random Forest model:     rf_helpdesk_true.joblib\n",
      "\n",
      "\n",
      "Parameters Random Forest:\n",
      "rf_n_estimators:         100\n",
      "rf_n_jobs:               -1\n",
      "\n",
      "Report has been written to 'reports/prediction/competing_artifacts/random_forest/2023-12-29_21-44-26_report_prediction_rf_helpdesk_true.txt'\n"
     ]
    }
   ],
   "source": [
    "# Generate report\n",
    "report_prediction_random_forest(filename_variables, variables, variables['rf_timestamp_prediction_start'], path_reports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store variables in pickle file\n",
    "with open(path_control + filename_variables, 'wb') as file:\n",
    "    pickle.dump(variables, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
